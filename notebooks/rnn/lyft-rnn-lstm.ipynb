{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# --- plotly ---\n",
    "from plotly import tools, subplots\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "# --- models ---\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "# --- setup ---\n",
    "pd.set_option('max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "\n",
    "import l5kit\n",
    "from l5kit.data import ChunkedDataset, LocalDataManager\n",
    "from l5kit.dataset import EgoDataset, AgentDataset\n",
    "\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\n",
    "from l5kit.geometry import transform_points\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from l5kit.data import PERCEPTION_LABELS\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "rc('animation', html='jshtml')\n",
    "print(\"l5kit version:\", l5kit.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
    "from keras.optimizers import SGD\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"/kaggle/input/lyft-motion-prediction-autonomous-vehicles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = LocalDataManager()\n",
    "dataset_path = dm.require('scenes/sample.zarr')\n",
    "zarr_dataset = ChunkedDataset(dataset_path)\n",
    "zarr_dataset.open()\n",
    "print(zarr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zarr_dataset.agents)\n",
    "print(zarr_dataset.agents.shape)\n",
    "n = zarr_dataset.agents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to convert a timedelta to a string (dropping milliseconds)\n",
    "def deltaToString(delta):\n",
    "    timeObj = time.gmtime(delta.total_seconds())\n",
    "    return time.strftime('%H:%M:%S', timeObj)\n",
    "\n",
    "class ProgressBar:\n",
    "    \n",
    "    # constructor\n",
    "    #   maxIterations: maximum number of iterations\n",
    "    def __init__(self, maxIterations):\n",
    "        self.maxIterations = maxIterations\n",
    "        self.granularity = 100 # 1 whole percent\n",
    "    \n",
    "    # start the timer\n",
    "    def start(self):\n",
    "        self.start = datetime.now()\n",
    "    \n",
    "    # check the progress of the current iteration\n",
    "    #   # currentIteration: the current iteration we are on\n",
    "    def check(self, currentIteration, chunked=False):\n",
    "        if currentIteration % round(self.maxIterations / self.granularity) == 0 or chunked:\n",
    "            \n",
    "            percentage = round(currentIteration / (self.maxIterations - self.maxIterations / self.granularity) * 100)\n",
    "            \n",
    "            current = datetime.now()\n",
    "            \n",
    "            # time calculations\n",
    "            timeElapsed = (current - self.start)\n",
    "            timePerStep = timeElapsed / (currentIteration + 1)\n",
    "            totalEstimatedTime = timePerStep * self.maxIterations\n",
    "            timeRemaining = totalEstimatedTime - timeElapsed\n",
    "            \n",
    "            # string formatting\n",
    "            percentageStr = \"{:>3}%  \".format(percentage)\n",
    "            remainingStr = \"Remaining: {}  \".format(deltaToString(timeRemaining))\n",
    "            elapsedStr = \"Elapsed: {}  \".format(deltaToString(timeElapsed))\n",
    "            totalStr = \"Total: {}\\r\".format(deltaToString(totalEstimatedTime))\n",
    "            \n",
    "            print(percentageStr + remainingStr + elapsedStr + totalStr, end=\"\")\n",
    "\n",
    "    def end(self):\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAgentsChunked(dataset, subsetPercent=1, chunks=10, mask_copy=[]):\n",
    "\n",
    "    datasetLength = round(len(dataset) * subsetPercent)\n",
    "    chunkSize = round(datasetLength / chunks)\n",
    "    \n",
    "    pb = ProgressBar(datasetLength)\n",
    "    pb.start()\n",
    "\n",
    "    agents = []\n",
    "    for i in range(0, datasetLength, chunkSize):\n",
    "\n",
    "        agentsSubset = dataset[i:i+chunkSize]\n",
    "        for j in range(0,len(agentsSubset)):\n",
    "            if len(mask_copy) > 0 and (j + i < len(mask_copy)) and not(mask_copy[i+j]):\n",
    "                continue\n",
    "            \n",
    "            agent = agentsSubset[j]\n",
    "            track_id = agent[4]\n",
    "\n",
    "            while track_id >= len(agents):\n",
    "                agents.append([])\n",
    "\n",
    "            data = []\n",
    "            centroid = agent[0]\n",
    "            yaw = agent[2]\n",
    "            velocity = agent[3]\n",
    "            data.append(centroid[0])\n",
    "            data.append(centroid[1])\n",
    "            data.append(yaw)\n",
    "            data.append(velocity[0])\n",
    "            data.append(velocity[1])\n",
    "            \n",
    "            agents[int(track_id)-1].append(data)\n",
    "            \n",
    "        pb.check(i, True)\n",
    "\n",
    "    return agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zarr_dataset.agents, \"\\n\")\n",
    "print(type(zarr_dataset.agents[0][0][0]))\n",
    "print(type(zarr_dataset.agents[0][0]))\n",
    "print(type(zarr_dataset.agents[0]))\n",
    "print(type(zarr_dataset.agents))\n",
    "agents = []\n",
    "print(type(agents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetPercent = 1*10**-1\n",
    "print(subsetPercent)\n",
    "agents = getAgentsChunked(zarr_dataset.agents, subsetPercent, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAgents(agents):\n",
    "    r = lambda: random.randint(0,255)\n",
    "    pb = ProgressBar(len(agents))\n",
    "    pb.start()\n",
    "    for i in range(0, len(agents)):\n",
    "        agent = agents[i]\n",
    "        centroid_x = []\n",
    "        centroid_y = []\n",
    "        for centroid in agent:\n",
    "            centroid_x.append(centroid[0])\n",
    "            centroid_y.append(centroid[1])\n",
    "        plt.plot(centroid_x, centroid_y, 'o', color='#%02X%02X%02X' % (r(),r(),r()))\n",
    "        pb.check(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAgents(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeAgents(agents):\n",
    "    dataForNormalization = []\n",
    "    pb = ProgressBar(len(agents))\n",
    "    pb.start()\n",
    "    for i in range(0, len(agents)):\n",
    "        agent = agents[i]\n",
    "        pb.check(i)\n",
    "        for data in agent:\n",
    "            for i in range(0, len(data)):\n",
    "                feature = data[i]\n",
    "                if i >= len(dataForNormalization):\n",
    "                    dataForNormalization.append([])\n",
    "                dataForNormalization[i].append(feature)\n",
    "        \n",
    "    \n",
    "    first = True\n",
    "    normalizedAgents = []\n",
    "    pb = ProgressBar(len(dataForNormalization) * len(agents))\n",
    "    counter = 0\n",
    "    pb.start()\n",
    "    for i in range(0, len(dataForNormalization)):\n",
    "        pb.end()\n",
    "        data = dataForNormalization[i]\n",
    "        min_ = np.min(data)\n",
    "        max_ = np.max(data)\n",
    "        print(\"max[{}]\".format(i),max_)\n",
    "        print(\"min[{}]\".format(i),min_,\"\\n\")\n",
    "        \n",
    "        for j in range(0, len(agents)):\n",
    "            counter = counter + 1\n",
    "            pb.check(counter)\n",
    "            if j >= len(normalizedAgents):\n",
    "                normalizedAgents.append([])\n",
    "                \n",
    "            agent = agents[j]\n",
    "            normalizedAgent = normalizedAgents[j]\n",
    "            \n",
    "            for k in range(0, len(agent)):\n",
    "                if k >= len(normalizedAgent):\n",
    "                    normalizedAgent.append([])\n",
    "                data = agent[k]\n",
    "                normalizedData = normalizedAgent[k]\n",
    "                \n",
    "                feature = data[i]\n",
    "                normalizedFeature = (feature - min_) / (max_ - min_)\n",
    "                if i == 0 and first:\n",
    "                    print(feature)\n",
    "                    print(normalizedFeature)\n",
    "                    first = False\n",
    "                \n",
    "                if i >= len(normalizedData):\n",
    "                    normalizedData.append(0)\n",
    "                normalizedData[i] = normalizedFeature\n",
    "    return normalizedAgents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizedAgents = normalizeAgents(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(agents))\n",
    "print(len(normalizedAgents),\"\\n\")\n",
    "\n",
    "print(agents[0][0][0])\n",
    "print(normalizedAgents[0][0][0],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAgentsInfo(agents, limit):\n",
    "    print(\"len(agents)\", len(agents), \"\\n\")\n",
    "\n",
    "    agentCentroidLengths = []\n",
    "    agentsOverLimit = []\n",
    "    for agent in agents:\n",
    "        agentCentroidLengths.append(len(agent))\n",
    "        if len(agent) > limit:\n",
    "            agentsOverLimit.append(agent)\n",
    "\n",
    "    print(\"len(agentCentroidLengths)\",len(agentCentroidLengths), \"\\n\")\n",
    "\n",
    "    print(\"max\",np.max(agentCentroidLengths))\n",
    "    print(\"min\",np.min(agentCentroidLengths))\n",
    "    print(\"mean\",np.mean(agentCentroidLengths))\n",
    "    print(\"std\",np.std(agentCentroidLengths), \"\\n\")\n",
    "\n",
    "    print(\"agents with {}+ history\".format(limit),len(agentsOverLimit))\n",
    "    return agentsOverLimit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 10\n",
    "agentsOverLimit = printAgentsInfo(normalizedAgents, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingSets(agents, limit):\n",
    "    allTrainingSets = []\n",
    "    totalNumberOfTrainingSets = 0\n",
    "    \n",
    "    pb = ProgressBar(len(agentsOverLimit))\n",
    "    pb.start()\n",
    "    for i in range(0, len(agentsOverLimit)):\n",
    "        agent = agentsOverLimit[i]\n",
    "        agentTrainingSets = []\n",
    "        for i in range(limit, len(agent)-1):\n",
    "            agentTrainingSet = []\n",
    "\n",
    "            start = i - limit\n",
    "            end = i\n",
    "            output = i + 1\n",
    "\n",
    "            agentTrainingSet.append(agent[start:end])\n",
    "            agentTrainingSet.append(agent[output])\n",
    "            agentTrainingSets.append(agentTrainingSet)\n",
    "\n",
    "            totalNumberOfTrainingSets = totalNumberOfTrainingSets + 1\n",
    "\n",
    "        allTrainingSets.append(agentTrainingSets)\n",
    "        pb.check(i)\n",
    "    pb.end()\n",
    "    \n",
    "    print(\"len(allTrainingSets)\", len(allTrainingSets))\n",
    "    print(\"len(allTrainingSets[0])\",len(allTrainingSets[0]), \"\\n\")\n",
    "\n",
    "    print(\"len(agentsOverLimit)\",len(agentsOverLimit))\n",
    "    print(\"len(agentsOverLimit[0]) - limit - 1\",len(agentsOverLimit[0]) - limit - 1, \"\\n\")\n",
    "\n",
    "    print(\"totalNumberOfTrainingSets\",totalNumberOfTrainingSets)\n",
    "    return allTrainingSets, totalNumberOfTrainingSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTrainingSets, totalNumberOfTrainingSets = getTrainingSets(agentsOverLimit, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(allTrainingSets))\n",
    "print(len(allTrainingSets[0]))\n",
    "print(len(allTrainingSets[0][0]))\n",
    "print(len(allTrainingSets[0][0][0]))\n",
    "print(len(allTrainingSets[0][0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenTrainingSets(allTrainingSets, totalNumberOfTrainingSets):\n",
    "    allTrainingSetsFlattened_X = np.empty((totalNumberOfTrainingSets, limit, len(allTrainingSets[0][0][0][0])))\n",
    "    allTrainingSetsFlattened_Y = np.empty((totalNumberOfTrainingSets, len(allTrainingSets[0][0][0][0])))\n",
    "    count = 0\n",
    "    for allTrainingSet in allTrainingSets:\n",
    "        for trainingSet in allTrainingSet:\n",
    "            allTrainingSetsFlattened_X[count] = np.array(trainingSet[0])\n",
    "            allTrainingSetsFlattened_Y[count] = trainingSet[1]\n",
    "            count = count + 1\n",
    "    print(\"len(allTrainingSetsFlattened_X)\", len(allTrainingSetsFlattened_X))\n",
    "    return allTrainingSetsFlattened_X, allTrainingSetsFlattened_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTrainingSetsFlattened_X, allTrainingSetsFlattened_Y = flattenTrainingSets(allTrainingSets, totalNumberOfTrainingSets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(allTrainingSetsFlattened_X)\n",
    "depth = len(allTrainingSetsFlattened_X[0])\n",
    "channels = len(allTrainingSetsFlattened_X[0][0])\n",
    "\n",
    "print(\"length\", length)\n",
    "print(\"depth\", depth)\n",
    "print(\"channels\",channels)\n",
    "print(\"length*depth*channels\",length*depth*channels)\n",
    "\n",
    "allTrainingSetsFlattened_Input = allTrainingSetsFlattened_X\n",
    "allTrainingSetsFlattened_Output = allTrainingSetsFlattened_Y\n",
    "\n",
    "print(allTrainingSetsFlattened_Input.shape[1])\n",
    "print(allTrainingSetsFlattened_Input.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LSTM architecture\n",
    "regressor = Sequential()\n",
    "# First LSTM layer with Dropout regularisation\n",
    "regressor.add(LSTM(units=50, return_sequences=True, input_shape=(allTrainingSetsFlattened_Input.shape[1],allTrainingSetsFlattened_Input.shape[2])))\n",
    "regressor.add(Dropout(0.2))\n",
    "# Second LSTM layer\n",
    "regressor.add(LSTM(units=50, return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "# Third LSTM layer\n",
    "regressor.add(LSTM(units=50, return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "# Fourth LSTM layer\n",
    "regressor.add(LSTM(units=50))\n",
    "regressor.add(Dropout(0.2))\n",
    "# The output layer\n",
    "regressor.add(Dense(units=allTrainingSetsFlattened_Input.shape[2]))\n",
    "\n",
    "# Compiling the RNN\n",
    "regressor.compile(optimizer='rmsprop',loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting to the training set\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Epoch: {}             loss: {}\\n\".format(self.epoch, logs['loss']), end=\"\")\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        if batch % 100 == 0:\n",
    "            print(\"Epoch: {} batchs: {}% loss: {}\\r\".format(self.epoch, round(batch / self.params['steps'] * 100), logs['loss']), end=\"\")\n",
    "\n",
    "regressor.fit(allTrainingSetsFlattened_Input,allTrainingSetsFlattened_Output,epochs=2,batch_size=128,verbose=0,callbacks=[CustomCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_test = dm.require('scenes/test.zarr')\n",
    "zarr_dataset_test = ChunkedDataset(dataset_path_test)\n",
    "zarr_dataset_test.open()\n",
    "print(zarr_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mask = np.load('../input/lyft-motion-prediction-autonomous-vehicles/scenes/mask.npz')[\"arr_0\"]\n",
    "print(test_mask)\n",
    "print(test_mask.shape)\n",
    "print(test_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetPercent = 1 * 10**-1\n",
    "subsetLength = round(len(test_mask) * subsetPercent)\n",
    "count = 0\n",
    "pb = ProgressBar(subsetLength)\n",
    "pb.start()\n",
    "chunkSize = 10\n",
    "mask_copy = []\n",
    "mask_indexes = []\n",
    "for i in range(0, subsetLength, chunkSize):\n",
    "    chunkedTestMask = test_mask[i: i + chunkSize]\n",
    "    for j in range(0, len(chunkedTestMask)):\n",
    "        mask = chunkedTestMask[j]\n",
    "        mask_copy.append(mask)\n",
    "        if mask:\n",
    "            mask_indexes.append(i + j)\n",
    "            count = count + 1\n",
    "    pb.check(i)\n",
    "pb.end()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = mask_indexes[0]\n",
    "diffs = []\n",
    "for i in range(1, len(mask_indexes)):\n",
    "    curr = mask_indexes[i]\n",
    "    diff = curr - prev\n",
    "    diffs.append(diff)\n",
    "    prev = curr\n",
    "print(diffs[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max\",np.max(diffs))\n",
    "print(\"min\",np.min(diffs))\n",
    "print(\"median\",np.median(diffs))\n",
    "print(\"mean\",np.mean(diffs))\n",
    "print(\"std\",np.std(diffs), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(zarr_dataset_test.agents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetPercent = 1*10**-1\n",
    "print(subsetPercent)\n",
    "agentsTest = getAgentsChunked(zarr_dataset_test.agents, subsetPercent, 1000, mask_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAgents(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizedAgentsTest = normalizeAgents(agentsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agentsTestOverLimit = printAgentsInfo(normalizedAgentsTest, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTestingSets, totalNumberOfTestingSets = getTrainingSets(agentsTestOverLimit, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTestingSetsFlattened_X, allTestingSetsFlattened_Y = flattenTrainingSets(allTestingSets, totalNumberOfTestingSets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTestingSetsFlattened_Input = allTestingSetsFlattened_X\n",
    "allTestingSetsFlattened_Output = allTestingSetsFlattened_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(allTestingSetsFlattened_Input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = len(allTestingSetsFlattened_Input)\n",
    "print(max)\n",
    "chunkSize = 100\n",
    "pb = ProgressBar(max)\n",
    "pb.start()\n",
    "predictedTestAgentCentroid = np.empty((1,5))\n",
    "for i in range(0, max-chunkSize, chunkSize):#len(zarr_dataset.agents)):\n",
    "    newPredictions = regressor.predict(allTestingSetsFlattened_Input[i:i+chunkSize])\n",
    "    print(newPredictions.shape)\n",
    "    predictedTestAgentCentroid = np.concatenate((predictedTestAgentCentroid, newPredictions))\n",
    "    pb.check(i, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictedTestAgentCentroid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictedTestAgentCentroid.shape)\n",
    "predictedTestAgentCentroid = predictedTestAgentCentroid[1:len(predictedTestAgentCentroid)]\n",
    "print(predictedTestAgentCentroid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predictedTestAgentCentroid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomSamples = 10\n",
    "for i in range(0, len(predictedTestAgentCentroid), round(len(predictedTestAgentCentroid) / randomSamples)):\n",
    "    testSet = allTestingSetsFlattened_Input[i]\n",
    "    lastTestSet = testSet[len(testSet[0]) - 1][0]\n",
    "    firstPrediction = predictedTestAgentCentroid[i][0]\n",
    "    print(lastTestSet)\n",
    "    print(firstPrediction,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"submission.csv\"\n",
    "testCSVOutput = np.empty((5,50,2))\n",
    "print(testCSVOutput.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(csv_path):\n",
    "    os.remove(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummyData = np.empty((10,50,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(csv_path, 'w')\n",
    "\n",
    "def printCoord(row, axis, confidence, timestep):\n",
    "    return row + \"coord_\" + axis + str(confidence) + str(timestep) + \",\"\n",
    "    \n",
    "# timestamp track_id conf_0 conf_1 conf_2\tcoord_x00 coord_y249\n",
    "row = \"\"\n",
    "row = row + \"timestamp\" + \",\"\n",
    "row = row + \"track_id\" + \",\"\n",
    "for i in range(0,3):\n",
    "    row = row + \"conf_\" + str(i) + \",\"\n",
    "for confidence in range(0,3):\n",
    "    for timestep in range(0,50):\n",
    "        row = printCoord(row, \"x\", confidence, timestep)\n",
    "        row = printCoord(row, \"y\", confidence, timestep)\n",
    "row = row + \"\\n\"\n",
    "print(row)\n",
    "file.write(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(dummyData)):\n",
    "    idRow = dummyData[i]\n",
    "    row = \"\"\n",
    "    row = str(i)\n",
    "    for future in idRow:\n",
    "        for pos in future:\n",
    "            row = row + str(pos) + \",\"\n",
    "    row = row + \"\\n\"\n",
    "    print(row)\n",
    "    file.write(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
